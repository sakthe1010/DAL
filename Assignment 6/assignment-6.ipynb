{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ME21B174 ASSIGNMENT 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the necessaary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : In the dataset I downloaded, there was no colummn named \"token\". So, For now I have taken other 3 columns as required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set of languages required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = [\"af-ZA\", \"cy-GB\", \"da-DK\", \"de-DE\", \"en-US\", \"es-ES\", \"fr-FR\", \"fi-FI\", \"hu-HU\", \"is-IS\", \"it-IT\", \"jv-ID\", \"lv-LV\", \"ms-MY\", \"nb-NO\", \"nl-NL\", \"pl-PL\", \"pt-PT\", \n",
    "\"ro-RO\", \"ru-RU\", \"sl-SL\", \"sv-SE\", \"sq-AL\", \"sw-KE\", \"tl-PH\", \"tr-TR\", \"vi-VN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the next part of the code, create a new folder named \"task1\" in the assignement folder. The following code will create a csv file for each language in the task1 folder and store the required data of each language in it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in languages:\n",
    "    \n",
    "    data = pd.read_json(f\"1.0/data/{lang}.jsonl\", lines=True)\n",
    "    df = data[\"utt\"]\n",
    "    df.to_csv(f\"task1/{lang}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a master dataframe which contains the required data from all 27 languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_req =  [\"locale\", \"partition\", \"utt\",] ## COLUMNS OF INTEREST\n",
    "master_df = pd.DataFrame()\n",
    "for lang in languages:\n",
    "    df = pd.read_json(f\"1.0/data/{lang}.jsonl\", lines=True)\n",
    "    df = df[columns_req]\n",
    "    master_df = pd.concat([master_df, df], axis=0) ## CONCAT\n",
    "\n",
    "master_df.reset_index(inplace=True, drop=True) ## RESETTING INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>locale</th>\n",
       "      <th>partition</th>\n",
       "      <th>utt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>af-ZA</td>\n",
       "      <td>test</td>\n",
       "      <td>maak my wakker om vyf v. m. die week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>af-ZA</td>\n",
       "      <td>train</td>\n",
       "      <td>maak my wakker nege-uur v. m. op vrydag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>af-ZA</td>\n",
       "      <td>train</td>\n",
       "      <td>stel 'n alarm vir twee ure van nou af</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>af-ZA</td>\n",
       "      <td>test</td>\n",
       "      <td>stil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>af-ZA</td>\n",
       "      <td>train</td>\n",
       "      <td>janneman stilte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446062</th>\n",
       "      <td>vi-VN</td>\n",
       "      <td>train</td>\n",
       "      <td>tôi có email nào không</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446063</th>\n",
       "      <td>vi-VN</td>\n",
       "      <td>train</td>\n",
       "      <td>những thư điện tử nào là mới</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446064</th>\n",
       "      <td>vi-VN</td>\n",
       "      <td>train</td>\n",
       "      <td>tôi có thư điện tử mới nào từ huy không</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446065</th>\n",
       "      <td>vi-VN</td>\n",
       "      <td>test</td>\n",
       "      <td>hùng có gửi tôi email không</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446066</th>\n",
       "      <td>vi-VN</td>\n",
       "      <td>train</td>\n",
       "      <td>kiểm tra email của huy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>446067 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       locale partition                                      utt\n",
       "0       af-ZA      test     maak my wakker om vyf v. m. die week\n",
       "1       af-ZA     train  maak my wakker nege-uur v. m. op vrydag\n",
       "2       af-ZA     train    stel 'n alarm vir twee ure van nou af\n",
       "3       af-ZA      test                                     stil\n",
       "4       af-ZA     train                          janneman stilte\n",
       "...       ...       ...                                      ...\n",
       "446062  vi-VN     train                   tôi có email nào không\n",
       "446063  vi-VN     train             những thư điện tử nào là mới\n",
       "446064  vi-VN     train  tôi có thư điện tử mới nào từ huy không\n",
       "446065  vi-VN      test              hùng có gửi tôi email không\n",
       "446066  vi-VN     train                   kiểm tra email của huy\n",
       "\n",
       "[446067 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use naive bayes classifier, we need to convert the features and label to numerical values. I have used label encoder for encoding label and countvectorizer to convert features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "master_df[\"locale\"] = encoder.fit_transform(master_df[\"locale\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  7,  6,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 22, 21, 23, 24, 25, 26])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df[\"locale\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train - test - val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = master_df[master_df[\"partition\"] == \"train\"][\"utt\"]\n",
    "y_train = master_df[master_df[\"partition\"] == \"train\"][\"locale\"]\n",
    "X_test = master_df[master_df[\"partition\"] == \"test\"][\"utt\"]\n",
    "y_test = master_df[master_df[\"partition\"] == \"test\"][\"locale\"]\n",
    "X_dev = master_df[master_df[\"partition\"] == \"dev\"][\"utt\"]\n",
    "y_dev = master_df[master_df[\"partition\"] == \"dev\"][\"locale\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer converts a collection of text documents into a matrix of token counts. It tokenizes the text, builds a vocabulary of unique words, and encodes each document as a sparse matrix where each entry represents the frequency of a word in that document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "X_dev = vectorizer.transform(X_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the optimal alpha for the naive bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha : 0.1, Best score : 0.9851159570785739\n"
     ]
    }
   ],
   "source": [
    "alphas = np.linspace(0.1, 1, 10)\n",
    "best_score = 0\n",
    "best_alpha = 0\n",
    "for alpha in alphas:\n",
    "    classifier = MultinomialNB(alpha=alpha)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    score = classifier.score(X_dev, y_dev)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_alpha = alpha\n",
    "print(f\"Best alpha : {best_alpha}, Best score : {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score : 0.9847567809908092\n"
     ]
    }
   ],
   "source": [
    "best_classifier = MultinomialNB(alpha=best_alpha)\n",
    "best_classifier.fit(X_train, y_train)\n",
    "print(f\"Test score : {best_classifier.score(X_test, y_test)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model performs well for this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating 4 language groups based on continent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asia\n",
    "asia = [\"jv-ID\", \"ms-MY\", \"tl-PH\", \"tr-TR\", \"vi-VN\"]\n",
    "\n",
    "# Africa\n",
    "africa = [\"af-ZA\", \"sw-KE\"]\n",
    "\n",
    "# Europe\n",
    "europe = [\"cy-GB\", \"da-DK\", \"de-DE\", \"es-ES\", \"fr-FR\", \"fi-FI\", \"hu-HU\", \"is-IS\", \"it-IT\", \n",
    "          \"lv-LV\", \"nb-NO\", \"nl-NL\", \"pl-PL\", \"pt-PT\", \"ro-RO\", \"ru-RU\", \"sl-SL\", \"sv-SE\", \"sq-AL\"]\n",
    "\n",
    "# North America\n",
    "north_america = [\"en-US\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findContinent(language): ## RETURN CONTINENT\n",
    "    if language in asia:\n",
    "        return \"Asia\"\n",
    "    elif language in africa:\n",
    "        return \"Africa\"\n",
    "    elif language in europe:\n",
    "        return \"Europe\"\n",
    "    elif language in north_america:\n",
    "        return \"North America\"\n",
    "    else:\n",
    "        return \"Unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a seperate column named \"continent\" which denotes the continent of each datapoint in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_req =  [\"locale\", \"partition\", \"utt\",]\n",
    "continental_df = pd.DataFrame()\n",
    "for lang in languages:\n",
    "    df = pd.read_json(f\"1.0/data/{lang}.jsonl\", lines=True)\n",
    "    df = df[columns_req]\n",
    "    df[\"Continent\"] = findContinent(lang)\n",
    "    continental_df = pd.concat([continental_df, df], axis=0)\n",
    "\n",
    "continental_df.reset_index(inplace=True, drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>locale</th>\n",
       "      <th>partition</th>\n",
       "      <th>utt</th>\n",
       "      <th>Continent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>af-ZA</td>\n",
       "      <td>test</td>\n",
       "      <td>maak my wakker om vyf v. m. die week</td>\n",
       "      <td>Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>af-ZA</td>\n",
       "      <td>train</td>\n",
       "      <td>maak my wakker nege-uur v. m. op vrydag</td>\n",
       "      <td>Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>af-ZA</td>\n",
       "      <td>train</td>\n",
       "      <td>stel 'n alarm vir twee ure van nou af</td>\n",
       "      <td>Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>af-ZA</td>\n",
       "      <td>test</td>\n",
       "      <td>stil</td>\n",
       "      <td>Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>af-ZA</td>\n",
       "      <td>train</td>\n",
       "      <td>janneman stilte</td>\n",
       "      <td>Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446062</th>\n",
       "      <td>vi-VN</td>\n",
       "      <td>train</td>\n",
       "      <td>tôi có email nào không</td>\n",
       "      <td>Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446063</th>\n",
       "      <td>vi-VN</td>\n",
       "      <td>train</td>\n",
       "      <td>những thư điện tử nào là mới</td>\n",
       "      <td>Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446064</th>\n",
       "      <td>vi-VN</td>\n",
       "      <td>train</td>\n",
       "      <td>tôi có thư điện tử mới nào từ huy không</td>\n",
       "      <td>Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446065</th>\n",
       "      <td>vi-VN</td>\n",
       "      <td>test</td>\n",
       "      <td>hùng có gửi tôi email không</td>\n",
       "      <td>Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446066</th>\n",
       "      <td>vi-VN</td>\n",
       "      <td>train</td>\n",
       "      <td>kiểm tra email của huy</td>\n",
       "      <td>Asia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>446067 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       locale partition                                      utt Continent\n",
       "0       af-ZA      test     maak my wakker om vyf v. m. die week    Africa\n",
       "1       af-ZA     train  maak my wakker nege-uur v. m. op vrydag    Africa\n",
       "2       af-ZA     train    stel 'n alarm vir twee ure van nou af    Africa\n",
       "3       af-ZA      test                                     stil    Africa\n",
       "4       af-ZA     train                          janneman stilte    Africa\n",
       "...       ...       ...                                      ...       ...\n",
       "446062  vi-VN     train                   tôi có email nào không      Asia\n",
       "446063  vi-VN     train             những thư điện tử nào là mới      Asia\n",
       "446064  vi-VN     train  tôi có thư điện tử mới nào từ huy không      Asia\n",
       "446065  vi-VN      test              hùng có gửi tôi email không      Asia\n",
       "446066  vi-VN     train                   kiểm tra email của huy      Asia\n",
       "\n",
       "[446067 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "continental_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seggregating the master dataset based on continents present "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "african_langs = continental_df[continental_df[\"Continent\"] == \"Africa\"].reset_index(inplace=False, drop=True)\n",
    "asian_langs = continental_df[continental_df[\"Continent\"] == \"Asia\"].reset_index(inplace=False, drop=True)\n",
    "european_langs = continental_df[continental_df[\"Continent\"] == \"Europe\"].reset_index(inplace=False, drop=True)\n",
    "north_american_langs = continental_df[continental_df[\"Continent\"] == \"North America\"].reset_index(inplace=False, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the next part of the code, create a new folder named \"task3\" in the assignement folder. The following code will create a csv file for each continent in the task3 folder and store the required data of each continent in it.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "african_langs.to_csv(\"task3/african.csv\")\n",
    "asian_langs.to_csv(\"task3/asian.csv\")\n",
    "european_langs.to_csv(\"task3/european.csv\")\n",
    "north_american_langs.to_csv(\"task3/north_american.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries for LDA and QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import (LinearDiscriminantAnalysis as LDA, QuadraticDiscriminantAnalysis as QDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN - TEST - VAL SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "task3_train = pd.concat([african_langs[african_langs[\"partition\"] == \"train\"], asian_langs[asian_langs[\"partition\"] == \"train\"],\n",
    "                         european_langs[european_langs[\"partition\"] == \"train\"], north_american_langs[north_american_langs[\"partition\"] == \"train\"]],\n",
    "                         axis=0).reset_index(inplace=False, drop=True)\n",
    "\n",
    "task3_test = pd.concat([african_langs[african_langs[\"partition\"] == \"test\"], asian_langs[asian_langs[\"partition\"] == \"test\"],\n",
    "                         european_langs[european_langs[\"partition\"] == \"test\"], north_american_langs[north_american_langs[\"partition\"] == \"test\"]],\n",
    "                         axis=0).reset_index(inplace=False, drop=True)\n",
    "\n",
    "task3_dev = pd.concat([african_langs[african_langs[\"partition\"] == \"dev\"], asian_langs[asian_langs[\"partition\"] == \"dev\"],\n",
    "                         european_langs[european_langs[\"partition\"] == \"dev\"], north_american_langs[north_american_langs[\"partition\"] == \"dev\"]],\n",
    "                         axis=0).reset_index(inplace=False, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = task3_train[\"utt\"]\n",
    "y_train = task3_train[\"Continent\"]\n",
    "\n",
    "X_test = task3_test[\"utt\"]\n",
    "y_test = task3_test[\"Continent\"]\n",
    "\n",
    "X_dev = task3_dev[\"utt\"]\n",
    "y_dev = task3_dev[\"Continent\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Tfidf vectorizer to create the feature space of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "t_vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train = t_vectorizer.fit_transform(X_train)\n",
    "X_test = t_vectorizer.transform(X_test)\n",
    "X_dev = t_vectorizer.transform(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<310878x152098 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1933920 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(310878, 152098)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have used SVD to do feature elimination to reduce from 152098 to 300 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=300, random_state=42)\n",
    "X_train = svd.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(310878, 300)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev = svd.transform(X_dev)\n",
    "X_test = svd.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearDiscriminantAnalysis(store_covariance=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearDiscriminantAnalysis</label><div class=\"sk-toggleable__content\"><pre>LinearDiscriminantAnalysis(store_covariance=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearDiscriminantAnalysis(store_covariance=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LDA(store_covariance=True)\n",
    "\n",
    "lda.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev score:  0.9316281357599606\n",
      "Test score:  0.9300480709357642\n"
     ]
    }
   ],
   "source": [
    "print(\"Dev score: \", lda.score(X_dev, y_dev))\n",
    "print(\"Test score: \", lda.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>QuadraticDiscriminantAnalysis(store_covariance=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">QuadraticDiscriminantAnalysis</label><div class=\"sk-toggleable__content\"><pre>QuadraticDiscriminantAnalysis(store_covariance=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "QuadraticDiscriminantAnalysis(store_covariance=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qda = QDA(store_covariance=True)\n",
    "\n",
    "qda.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev score:  0.9044287770308429\n",
      "Test score:  0.9041196542877781\n"
     ]
    }
   ],
   "source": [
    "print(\"Dev score: \", qda.score(X_dev, y_dev))\n",
    "print(\"Test score: \", qda.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis to prepare the RDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 300, 300)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(qda.covariance_).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 300)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(lda.covariance_).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the classes have same covariance matrix (Assumption taken in LDA) whereas each class have different covariance matrix in QDA. Hence, there is diffference in shape of the covariance matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 300)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.means_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 300)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qda.means_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07407407, 0.18518519, 0.7037037 , 0.03703704])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.priors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07407407, 0.18518519, 0.7037037 , 0.03703704])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qda.priors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.det(qda.covariance_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OBSERVATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that both lda and qda has same means and priors. But the determinant of covariance matrix of dataset is 0 as we have used 300 features which is still a bit large.  So , in the following RDA, no. of features have been reduced to 15. This is optimal for RDA even though it might not be enough features for qda to work properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN - TEST - VAL SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = task3_train[\"utt\"]\n",
    "y_train = task3_train[\"Continent\"]\n",
    "\n",
    "X_test = task3_test[\"utt\"]\n",
    "y_test = task3_test[\"Continent\"]\n",
    "\n",
    "X_dev = task3_dev[\"utt\"]\n",
    "y_dev = task3_dev[\"Continent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "t_vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train = t_vectorizer.fit_transform(X_train)\n",
    "X_test = t_vectorizer.transform(X_test)\n",
    "X_dev = t_vectorizer.transform(X_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd2 = TruncatedSVD(n_components=15, random_state=42) ## 15 features taken\n",
    "X_train = svd2.fit_transform(X_train)\n",
    "X_dev = svd2.transform(X_dev)\n",
    "X_test = svd2.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(310878, 15)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATING A CLASS FOR RDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RDA:\n",
    "    def __init__(self, lam=0.5) -> None:\n",
    "        self.lam = lam\n",
    "\n",
    "    def fit(self, X, y): ## FINDS THE OUTPUTS OF BOTH LDA AND QDA\n",
    "        self.classes_ = np.unique(y)\n",
    "        lda = LDA(store_covariance=True)\n",
    "        qda = QDA(store_covariance=True)\n",
    "        \n",
    "        lda.fit(X, y)\n",
    "        qda.fit(X, y)\n",
    "\n",
    "        qda_cov = np.array(qda.covariance_)\n",
    "        lda_cov = np.stack([np.array(lda.covariance_)] * qda_cov.shape[0])\n",
    "\n",
    "        ##  lam is the tradeoff between LDA and QDA\n",
    "\n",
    "        self.covariance_ = (1-self.lam) * np.array(qda_cov) + self.lam * np.array(lda_cov) \n",
    "        self.means_ = lda.means_           # Both LDA and QDA have same means \n",
    "        self.priors_ = lda.priors_          # Both LDA and QDA have same priors\n",
    "\n",
    "    ## The function calculates the log of the likelihood for each class using the Gaussian formula and adds the log of the prior probabilities.\n",
    "    ## It selects the class with the highest score as the predicted class for each sample.\n",
    "    def predict(self, X):\n",
    "        n, d = X.shape\n",
    "        self.scores = np.zeros((n, len(self.priors_)))\n",
    "        self.det_cov = np.linalg.det(self.covariance_)\n",
    "        self.inv_cov = np.linalg.inv(self.covariance_)\n",
    "    \n",
    "        for idx, cl in enumerate(self.classes_):\n",
    "            mean_vec = self.means_[idx]\n",
    "\n",
    "            diff = X - mean_vec \n",
    "            score = -0.5 * np.sum(diff @ self.inv_cov[idx] * diff, axis=1)  \n",
    "            score -= 0.5 * np.log(self.det_cov[idx])\n",
    "            score += np.log(self.priors_[idx])\n",
    "            self.scores[:, idx] = score\n",
    "\n",
    "        return self.classes_[np.argmax(self.scores, axis=1)]\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X_test)\n",
    "        correct_predictions = np.sum(y_pred == y_test)\n",
    "\n",
    "        accuracy = correct_predictions / len(y_test)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding optimal lambda for RDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1. ]\n"
     ]
    }
   ],
   "source": [
    "lambdas = np.linspace(0.0, 1.0, 11)\n",
    "print(lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lamda: 0.0, Score:0.5682831452838177\n",
      "Lamda: 0.1, Score:0.6158061221948243\n",
      "Lamda: 0.2, Score:0.8659244314926897\n",
      "Lamda: 0.30000000000000004, Score:0.860581832673292\n",
      "Lamda: 0.4, Score:0.8549278935963536\n",
      "Lamda: 0.5, Score:0.8517024085282323\n",
      "Lamda: 0.6000000000000001, Score:0.8488505317691599\n",
      "Lamda: 0.7000000000000001, Score:0.8468205932899948\n",
      "Lamda: 0.8, Score:0.8449525517447508\n",
      "Lamda: 0.9, Score:0.8432713143540312\n",
      "Lamda: 1.0, Score:0.8412040150439613\n",
      "Best Lambda: 0.2\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "best_lam = 0\n",
    " \n",
    "for lam in lambdas:\n",
    "    rda = RDA(lam=lam)\n",
    "    rda.fit(X_train, y_train)\n",
    "    print(f\"Lamda: {lam}, Score:{rda.score(X_test, y_test)}\")\n",
    "\n",
    "    if rda.score(X_test, y_test) > best_score:\n",
    "        best_score = rda.score(X_test, y_test)\n",
    "        best_lam = lam\n",
    "\n",
    "print(f\"Best Lambda: {best_lam}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score for RDA: 0.8659244314926897\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best Score for RDA: {best_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
